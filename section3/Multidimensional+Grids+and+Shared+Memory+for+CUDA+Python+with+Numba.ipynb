{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Grids and Shared Memory for CUDA Python with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will introduce a few more intermediate techniques for programming in CUDA with Numba. First we will introduce how CUDA provides the ability to create 2 and 3 dimensional blocks and grids which can ease programming when working with 2 and 3 dimensional data. Next, we will introduce an on-chip, programmer managed memory space called **shared memory** which can be used for very fast communication between threads within a given block. You will have a chance to use both of these techniques to optimize a 2D matrix multiplication kernel.\n",
    "\n",
    "This section also provides an appendix with an example of reducing **shared memory bank conflicts** for a matrix transpose algorithm, with a link to resources for further study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the time you complete the required parts of this section you will be able to:\n",
    "\n",
    "* Do GPU accelerated parallel work on multidimensional data sets using multi dimensional blocks and grids.\n",
    "* Use shared memory to cache data on chip and reduce slow global memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 and 3 Dimensional Blocks and Grids\n",
    "\n",
    "Both grids and blocks can be configured to contain a 2 or 3 dimensional collection of blocks or threads, respectively. This is done mostly as a matter of convenience for programmers who often work with 2 or 3 dimensional datasets. Here is a very trivial example to highlight the syntax. You may need to read *both* the kernel definition and its launch before the concept makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros(16).reshape(4, 4).astype(np.int32)\n",
    "d_A = cuda.to_device(A)\n",
    "\n",
    "@cuda.jit\n",
    "def add_2D_coordinates(A):\n",
    "    # By passing `2`, we get the thread's unique x and y coordinates in the 2D grid\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    A[y][x] = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Here we create a 2D grid with 4 blocks in a 2x2 structure, each with 4 threads in a 2x2 structure\n",
    "# by using a Python tuple to signify grid and block dimensions.\n",
    "blocks = (2, 2)\n",
    "threads_per_block = (2, 2)\n",
    "\n",
    "add_2D_coordinates[blocks, threads_per_block](d_A)\n",
    "print(d_A.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside About Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important always to remember that GPUs perform well when they are given large enough grids to support SMs in always having additional work to do while they wait on operations with latency to expire. With that in mind it's worth mentioning that several of the exercises in this section will ask you to write kernels that do not follow this best practice. This is done to give you a chance to focus on the syntax and some of the basic patterns of working in multiple dimensions, which tends to take a bit of orientation, even without doing meaningful work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add 2D Matrices on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will modify a kernel definition and its launch configuration to perform 2D matrix addition in parallel. To start with you'll be writing a naive kernel that will only work when launched with a grid whose shape matches that of the data. If you get stuck feel free to refer to [the solution](../../../../edit/tasks/task3/task/solutions/add_matrix_solution.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function is written to add two 1D vectors. Refactor it to add 2D matrices\n",
    "@cuda.jit\n",
    "def add_matrix(A, B, C):\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    C[i] = A[i] + B[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_matrix(A, B, C):\n",
    "    i, j = cuda.grid(2)\n",
    "    C[i][j]  = A[i][j] + B[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the values in this cell, which defines 2D matrices of size 36*36\n",
    "A = np.arange(36*36).reshape(36, 36).astype(np.int32)\n",
    "B = A * 2\n",
    "C = np.zeros_like(A)\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor the launch configuration to use a 2D grid with 2D blocks\n",
    "blocks = 36\n",
    "threads_per_block = 36\n",
    "\n",
    "# This launch will throw a Typing error until refactor the definition above to operate on 2D arrays\n",
    "add_matrix[blocks, threads_per_block](d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (6, 6)\n",
    "threads_per_block = (6, 6)\n",
    "\n",
    "# This launch will throw a Typing error until refactor the definition above to operate on 2D arrays\n",
    "add_matrix[blocks, threads_per_block](d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "output = d_C.copy_to_host()\n",
    "solution = A+B\n",
    "# This assertion will fail unles the output and solution are equal\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Multiply 2D Matrices on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you'll complete the logic of a kernel that calculates one element for a 2D [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) operation. Like the matrix add kernel you just wrote, this kernel is also naive in that it requires grid dimensions to match that of the passed in matrices. Refer to the [solution](../../../../edit/tasks/task3/task/solutions/matrix_multiply_solution.py) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def mm(a, b, c):\n",
    "    row, column = cuda.grid(2)\n",
    "    sum = 0\n",
    "    \n",
    "    ###\n",
    "    # TODO: Build the rest of this kernel to calculate the value for one element in the output matrix.\n",
    "    ###\n",
    "    for i in range(a.shape[1]):\n",
    "        sum += a[row][i] * b[i][column]\n",
    "    c[row][column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the values in this cell\n",
    "a = np.arange(16).reshape(4,4).astype(np.int32)\n",
    "b = np.arange(16).reshape(4,4).astype(np.int32)\n",
    "c = np.zeros_like(a)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "grid = (2,2)\n",
    "block = (2,2)\n",
    "mm[grid, block](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you successfully implement the kernel\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Striding in Multiple Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we can use Numba's `cuda.gridsize(1)` to obtain the total number of threads in a grid, so too we can use `cuda.gridsize(2)` to get variables for the total number of threads in each each direction for a 2D grid. This can be useful, for example, when we have a 2D dataset larger than our grid where we would wish for each thread to stride over the grid in a loop in order to cover all necessary work.\n",
    "\n",
    "Just like with our 1D grid stride loop, this technique also allows flexibility in the sizes of our grids and blocks, regardless of the shape of our data. The following example demonstrates the use of a 2D grid stride loop, ultimately printing information about which threads in the grid are working on which elements in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "@cuda.jit\n",
    "def add_2D_coordinates_stride(A):\n",
    "\n",
    "    grid_y, grid_x = cuda.grid(2)\n",
    "    # By passing `2`, we get the grid size in both the x an y dimensions\n",
    "    stride_y, stride_x = cuda.gridsize(2)\n",
    "    \n",
    "    for data_i in range(grid_x, A.shape[0], stride_x):\n",
    "        for data_j in range(grid_y, A.shape[1], stride_y):\n",
    "            A[data_i][data_j] = grid_x + grid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a 2D grid with 6 blocks in a 3x2 structure, each with 6 threads in a 3x2 structure. The grid is both smaller than our total data set, and has a shape that does not fit evenly into the dataset's dimensions. Still, the kernel is able to access each element in the data. After running the cell, play around with both the data's shape, as the grid's. Try to predict what the output matrix's values will be before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]\n",
      " [3 4 5 6 7]\n",
      " [0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]\n",
      " [3 4 5 6 7]\n",
      " [0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros(55).reshape(11, 5).astype(np.int32)\n",
    "d_A = cuda.to_device(A)\n",
    "\n",
    "blocks = (3, 2)\n",
    "threads_per_block = (3, 2)\n",
    "\n",
    "# With this configuration, `stride_x` will be 9, and `stride_y` will be 4\n",
    "add_2D_coordinates_stride[blocks, threads_per_block](d_A)\n",
    "print(d_A.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add 2D Matrices Larger than the Grid Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will modify the naive matrix addition kernel above to be able to work on datasets of an arbitrary size. If you get stuck feel free to refer to [the solution](../../../../edit/tasks/task3/task/solutions/add_matrix_stride_solution.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently this kernel will only work correctly when passed matrices that are of the same size as the grid.\n",
    "# Refactor using a strid in 2D so that it can work on data sets of an arbitrary size.\n",
    "@cuda.jit\n",
    "def add_matrix_stride(A, B, C):\n",
    "    j,i = cuda.grid(2)\n",
    "    \n",
    "    C[i,j] = A[i,j] + B[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_matrix_stride(A, B, C):\n",
    "    y, x = cuda.grid(2)\n",
    "    sy, sx = cuda.gridsize(2)\n",
    "    \n",
    "    for i in range(x, A.shape[0], sx):\n",
    "        for j in range(y, A.shape[1], sy):\n",
    "            C[i,j] = A[i,j] + B[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't modify the values in this cell. They create a scenario where the data is\n",
    "# larger than the grid size.\n",
    "A = np.arange(64*64).reshape(64, 64).astype(np.int32)\n",
    "B = A * 2\n",
    "C = np.zeros_like(A)\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)\n",
    "\n",
    "blocks = (6,6)\n",
    "threads_per_block = (6,6)\n",
    "\n",
    "add_matrix_stride[blocks, threads_per_block](d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "output = d_C.copy_to_host()\n",
    "solution = A+B\n",
    "# This assertion will fail unles the output and solution are equal\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Multiply 2D Matrices Larger than the Grid Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will complete a matrix multiplication kernel that will be able to work with arbitrary grid and data set shapes. You only need to work on the two lines containing the `TODO`s, using the `grid_` and `stride_` values to correctly map the work of the executing kernel into the data. Refer to the [solution](../../../../edit/tasks/task3/task/solutions/matrix_multiply_stride_solution.py) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "@cuda.jit\n",
    "def mm_stride(A, B, C):\n",
    "\n",
    "    grid_row, grid_column = cuda.grid(2)\n",
    "    stride_row, stride_column = cuda.gridsize(2)\n",
    "    \n",
    "    for data_row in range(0): # TODO: replace 0 with values that will correctly set data_row\n",
    "        for data_column in range(0): # TODO: replace 0 with values that will correctly set data_column\n",
    "            sum = 0\n",
    "            for i in range(A.shape[1]): # B.shape[0] would also be okay here\n",
    "                sum += A[data_row][i] * B[i][data_column]\n",
    "                \n",
    "            C[data_row][data_column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "@cuda.jit\n",
    "def mm_stride(A, B, C):\n",
    "\n",
    "    grid_column, grid_row = cuda.grid(2)\n",
    "    stride_column, stride_row = cuda.gridsize(2)\n",
    "    \n",
    "    for data_row in range(grid_row, A.shape[0], stride_row): # TODO: replace 0 with values that will correctly set data_row\n",
    "        for data_column in range(grid_column, B.shape[1], stride_column): # TODO: replace 0 with values that will correctly set data_column\n",
    "            sum = 0\n",
    "            for i in range(A.shape[1]): # B.shape[0] would also be okay here\n",
    "                sum += A[data_row][i] * B[i][data_column]\n",
    "                \n",
    "            C[data_row][data_column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not modify this cell. The strange dimensions of this data, and\n",
    "# the grid below are being set to make sure your kernel correctly handles arbitrary\n",
    "# data and grid sizes.\n",
    "\n",
    "a = np.arange(12).reshape(3,4).astype(np.int32)\n",
    "b = np.arange(24).reshape(4,6).astype(np.int32)\n",
    "c = np.zeros((a.shape[0], b.shape[1])).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "ts = (4, 3)\n",
    "bs = (3, 7)\n",
    "\n",
    "mm_stride[bs, ts](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory\n",
    "\n",
    "So far we have been differentiating between host and device memory, as if device memory were a single kind of memory. But in fact, CUDA has an even more fine-grained [memory hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy). The device memory we have been utilizing thus far is called **global memory** which is available to any thread or block on the device, can persist for the lifetime of the application, and is a relatively large memory space.\n",
    "\n",
    "As a final topic we will discuss how to utilize a region of on-chip device memory called **shared memory**. Shared memory is a programmer defined cache of limited size that [depends on the GPU](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) being used and is **shared** between all threads in a block. It is a scarce resource, cannot be accessed by threads outside of the block where it was allocated, and does not persist after a kernel finishes executing. Shared memory however has a much higher bandwidth than global memory and can be used to great effect in many kernels, especially to optimize performance.\n",
    "\n",
    "Here are a few common use cases for shared memory:\n",
    "\n",
    " * Caching memory read from global memory that will need to be read multiple times within a block.\n",
    " * Buffering output from threads so it can be coalesced before writing it back to global memory.\n",
    " * Staging data for scatter/gather operations within a block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory Syntax\n",
    "\n",
    "Numba provides [functions](https://numba.pydata.org/numba-doc/dev/cuda/memory.html#shared-memory-and-thread-synchronization) for allocating shared memory as well as for synchronizing between threads in a block, which is often necessary after parallel threads read from or write to shared memory.\n",
    "\n",
    "When declaring shared memory, you provide the shape of the shared array, as well as its type, using a [Numba type](https://numba.pydata.org/numba-doc/dev/reference/types.html#numba-types). **The shape of the array must be a constant value**, and therefore, you cannot use arguments passed into the function, or, provided variables like `numba.cuda.blockDim.x`, or the calculated values of `cuda.griddim`. Here is a convoluted example to demonstrate the syntax with comments pointing out the movement from host memory to global device memory, to shared memory, back to global device memory, and finally back to host memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import types, cuda\n",
    "\n",
    "@cuda.jit\n",
    "def swap_with_shared(x, y):\n",
    "    # Allocate a 4 element vector containing int32 values in shared memory.\n",
    "    temp = cuda.shared.array(4, dtype=types.int32)\n",
    "    \n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    # Move an element from global memory into shared memory\n",
    "    temp[idx] = x[idx]\n",
    "    \n",
    "    # cuda.syncthreads will force all threads in the block to synchronize here, which is necessary because...\n",
    "    cuda.syncthreads()\n",
    "    #...the following operation is reading an element written to shared memory by another thread.\n",
    "    \n",
    "    # Move an element from shared memory back into global memory\n",
    "    y[idx] = temp[cuda.blockDim.x - cuda.threadIdx.x - 1] # swap elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(4).astype(np.int32)\n",
    "y = np.zeros_like(x)\n",
    "\n",
    "# Move host memory to device (global) memory\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "swap_with_shared[1, 4](d_x, d_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move device (global) memory back to the host\n",
    "d_y.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Shared Memory Example: Matrix Transposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of the power of shared memory, and as a further example in 2D CUDA programming, let's write a matrix transpose kernel that takes a 2D array in row-major order and puts it in column-major order. (This is based on Mark Harris' [Efficient Matrix Transpose](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/) blog post, but uses a slightly simpler algorithm).\n",
    "\n",
    "In this example, we will be using shared memory as a mechanism to allow both coalesced reads and coalesced writes to and from global memory. This would typically not be possible for a transposition algorithm, but because shared memory access is so fast, we can make \"non-coalesced\" reads from or writes to it with no performance penalty in order to then read and/or write to or from global memory in a coalesced manner. The example will demonstrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Transposition Without Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before showing the shared memory implementation, let's first do a naive approach where we let each thread read and write individual elements independently using only global memory. In this way, when you see the shared memory example, you can focus on how shared memory impacts the algorithm, as opposed to the basics of the algorithm itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def transpose(a_in, a_out):\n",
    "    # Explicitly calculate indices rather than using cuda.grid(2)\n",
    "    row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "\n",
    "    a_out[row, col] = a_in[col, row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        0         1         2 ...     16381     16382     16383]\n",
      " [    16384     16385     16386 ...     32765     32766     32767]\n",
      " [    32768     32769     32770 ...     49149     49150     49151]\n",
      " ...\n",
      " [268386304 268386305 268386306 ... 268402685 268402686 268402687]\n",
      " [268402688 268402689 268402690 ... 268419069 268419070 268419071]\n",
      " [268419072 268419073 268419074 ... 268435453 268435454 268435455]]\n"
     ]
    }
   ],
   "source": [
    "size = 16384\n",
    "a_in = cuda.to_device(np.arange(size*size, dtype=np.int32).reshape((size, size)))\n",
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "print(a_in.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = (32, 32)\n",
    "blocks_per_grid = (int(size/threads_per_block[0]), int(size/threads_per_block[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.7 ms ± 93 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit transpose[blocks_per_grid, threads_per_block](a_in, a_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        0     16384     32768 ... 268386304 268402688 268419072]\n",
      " [        1     16385     32769 ... 268386305 268402689 268419073]\n",
      " [        2     16386     32770 ... 268386306 268402690 268419074]\n",
      " ...\n",
      " [    16381     32765     49149 ... 268402685 268419069 268435453]\n",
      " [    16382     32766     49150 ... 268402686 268419070 268435454]\n",
      " [    16383     32767     49151 ... 268402687 268419071 268435455]]\n"
     ]
    }
   ],
   "source": [
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Transposition With Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the algorithm using shared memory. To do so:\n",
    "\n",
    "1. Each block will create a 32x32 element shared memory array\n",
    "2. Each block will make a coalesced read from the input array in global memory into the shared memory array\n",
    "3. Before writing the values back to global memory, each thread in the block will wait for all other threads in the block to complete their reads using a thread sync\n",
    "4. We make a coalesced write to global memory from shared memory, writing in a transposed order. We will do this in two steps, first transposing the location of the shared memory tile within the total input array (4a below), and then, transposing the locations of the elements within the tile before writing them back to global memory (4b below).\n",
    "\n",
    "This method allows us to make the coalesced reads and writes to and from global memory since we can perform the transposition within the shared memory space where there is no performance penalty for non-contiguous reads and writes within shared memory arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.types\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose(a_in, a_out):\n",
    "    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n",
    "    # and that `a_in` is a multiple of these dimensions.\n",
    "    \n",
    "    # 1) Create 32x32 shared memory array.\n",
    "    tile = cuda.shared.array((32, 32), numba.types.int32)\n",
    "\n",
    "    # Compute offsets into global input array.\n",
    "    row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    \n",
    "    # 2) Make coalesced read from global memory into shared memory array.\n",
    "    # Note the use of local thread indices for the shared memory write,\n",
    "    # and global offsets for global memory read.\n",
    "    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a_in[col, row]\n",
    "\n",
    "    # 3) Wait for all threads in the block to finish updating shared memory.\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # 4a) Calculate transposed location for the shared memory array tile\n",
    "    # to be written back to global memory...\n",
    "    row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n",
    "\n",
    "    # 4b) ...Write back to global memory,\n",
    "    # transposing each element within the shared memory array.\n",
    "    a_out[col, row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.2 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "[[        0     16384     32768 ... 268386304 268402688 268419072]\n",
      " [        1     16385     32769 ... 268386305 268402689 268419073]\n",
      " [        2     16386     32770 ... 268386306 268402690 268419074]\n",
      " ...\n",
      " [    16381     32765     49149 ... 268402685 268419069 268435453]\n",
      " [    16382     32766     49150 ... 268402686 268419070 268435454]\n",
      " [    16383     32767     49151 ... 268402687 268419071 268435455]]\n"
     ]
    }
   ],
   "source": [
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "%timeit tile_transpose[blocks_per_grid, threads_per_block](a_in, a_out); cuda.synchronize()\n",
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a decent speed up on top of already accelerated code. (The [Efficient Matrix Transpose](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/) blog post demonstrates a method to achieve an even higher speedup by having a thread in the block be responsible for multiple entries in a tile, which reduces the cost of indexing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "The following exercise will require you to utilize everything you've learned so far. Unlike previous exercises, there will not be any solution code available to you, and, there are a couple additional steps you will need to take to \"run the assessment\" and get a score for your attempt(s). **Please read the directions carefully before beginning your work to ensure the best chance at successfully completing the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run the Assessment\n",
    "\n",
    "Take the following steps to complete this assessment:\n",
    "\n",
    "1. Using the instructions that follow, work on the cells below as you usually would for an exercise.\n",
    "2. When you are satisfied with your work, follow the instructions below to copy and paste code in into linked source code files. Be sure to save the files after you paste your work.\n",
    "3. Return to the browser tab you used to launch this notebook, and click on the **\"Assess\"** button. After a few seconds a score will be generated along with a helpful message.\n",
    "\n",
    "You are welcome to click on the **Assess** button as many times as you like, so feel free if you don't pass the first time to make additional modifications to your code and repeat steps 1 through 3. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiply with Shared Memory\n",
    "\n",
    "In this exercise you will complete a matrix mulitply kernel that will use shared memory to cache values from the input matrices so that they only need be accessed from global memory once, after which calculations for a thread's output element can utilize the cached values.This purpose of this assessment is to test your ability to reason about a 2D parallel problem and utilize shared memory. This particular problem doesn't have a ton of arithmetic intensity,  and we are not going to use a huge dataset so we will likely not see big speedups vs. the very simple CPU version. However, the ability to use the techniques asked of you will provide you ability in a wide number of situations where you will genuinely wish to accelerate some program involving a 2D dataset.\n",
    "\n",
    "To keep the focus on shared memory, this problem assumes input vectors of MxN and NxM dimensions with NxN threads per block and M/N blocks per grid. This means that shared memory caches with elements equal to the number of threads per block will be sufficient to provide all elements from the input matrices necessary for the calculations, and that no grid striding will be required.\n",
    "\n",
    "The following images shows the input matrices, the output matrix, a region of the output matrix that a block will calculate values for, the regions in the input matrices that this block will cache, and also, the output element and input elements for a single thread in that block:\n",
    "\n",
    "![matrix multiply diagram](images/mm_image.png)\n",
    "\n",
    "The shared memory caches have already been allocated in the kernel, your task is twofold:\n",
    "1. Use each thread in the block to populate one element in each of the caches.\n",
    "2. Use the shared memory caches in calculating each thread's `sum` value.\n",
    "\n",
    "Be sure to do any thread synchronizing that might be required to avoid cached values written by other threads not yet being available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave the values in this cell alone\n",
    "M = 128\n",
    "N = 32\n",
    "\n",
    "# Input vectors of MxN and NxM dimensions\n",
    "a = np.arange(M*N).reshape(M,N).astype(np.int32)\n",
    "b = np.arange(M*N).reshape(N,M).astype(np.int32)\n",
    "c = np.zeros((M, M)).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "# NxN threads per block, in 2 dimensions\n",
    "block_size = (N,N)\n",
    "# MxM/NxN blocks per grid, in 2 dimensions\n",
    "grid_size = (int(M/N),int(M/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making any modifications to `mm_shared` in the cell below, and before running the assessment, paste this cell's content into [**`assessment/definition.py`**](../../../../edit/tasks/task3/task/assessment/definition.py)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from numba import cuda, types\n",
    "@cuda.jit\n",
    "def mm_shared(a, b, c):\n",
    "    column, row = cuda.grid(2)\n",
    "    sum = 0\n",
    "\n",
    "    # `a_cache` and `b_cache` are already correctly defined\n",
    "    a_cache = cuda.shared.array(block_size, types.int32)\n",
    "    b_cache = cuda.shared.array(block_size, types.int32)\n",
    "\n",
    "    # TODO: use each thread to populate one element each a_cache and b_cache\n",
    "    \n",
    "    for i in range(a.shape[1]):\n",
    "        # TODO: calculate the `sum` value correctly using values from the cache \n",
    "        sum += a_cache[0][0] * b_cache[0][0]\n",
    "        \n",
    "    c[row][column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My answer-1, finished\n",
    "\n",
    "@cuda.jit\n",
    "def mm_shared(A, B, C):\n",
    "    \"\"\"\n",
    "    使用Shared Memory的矩阵乘法 C = A * B\n",
    "    \"\"\"\n",
    "    # 在Shared Memory中定义向量\n",
    "    # 向量可被整个Block的所有Thread共享\n",
    "    # 必须声明向量大小和数据类型\n",
    "    sA = cuda.shared.array(block_size, types.int32)\n",
    "    sB = cuda.shared.array(block_size, types.int32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    row = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    col = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "    \n",
    "    if row >= C.shape[0] and col >= C.shape[1]:\n",
    "        # 当(x, y)越界时退出\n",
    "        return\n",
    "\n",
    "    tmp = 0\n",
    "    BLOCK_SIZE = block_size[0]\n",
    "    # 以一个 BLOCK_SIZE x BLOCK_SIZE 为单位\n",
    "    for m in range(math.ceil(A.shape[1] / BLOCK_SIZE)):\n",
    "        sA[tx, ty] = A[row, ty + m * BLOCK_SIZE]\n",
    "        sB[tx, ty] = B[tx + m * BLOCK_SIZE, col]\n",
    "        # 线程同步，等待Block中所有Thread预加载结束\n",
    "        # 该函数会等待所有Thread执行完之后才执行下一步\n",
    "        cuda.syncthreads()\n",
    "        # 此时已经将A和B的子矩阵拷贝到了sA和sB\n",
    "\n",
    "        # 计算Shared Memory中的向量点积\n",
    "        # 直接从Shard Memory中读取数据的延迟很低\n",
    "        for n in range(BLOCK_SIZE):\n",
    "            tmp += sA[tx, n] * sB[n, ty]\n",
    "\n",
    "        # 线程同步，等待Block中所有Thread计算结束\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    # 循环后得到每个BLOCK的点积之和\n",
    "    C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Answer-2, trick\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, types\n",
    "@cuda.jit\n",
    "def mm_shared(a, b, c):\n",
    "    col, row = cuda.grid(2)\n",
    "    stride_col, stride_row = cuda.gridsize(2)\n",
    "\n",
    "    # `a_cache` and `b_cache` are already correctly defined\n",
    "    a_cache = cuda.shared.array(block_size, types.int32)\n",
    "    b_cache = cuda.shared.array(block_size, types.int32)\n",
    "\n",
    "    for data_row in range(row, a.shape[0], stride_row):\n",
    "        for data_col in range(col, b.shape[1], stride_col):\n",
    "            sum = 0\n",
    "            for i in range(a.shape[1]):\n",
    "                sum += a[data_row][i] * b[i][data_col]\n",
    "                \n",
    "            c[data_row][data_col] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's no need to update this kernel launch\n",
    "mm_shared[grid_size, block_size](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the contents in this cell\n",
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Now that you have completed this section you are able to:\n",
    "\n",
    "* Do GPU accelerated parallel work on multidimensional data sets using multi dimensional blocks and grids.\n",
    "* * Use shared memory to cache data on chip and reduce slow global memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Content\n",
    "\n",
    "To download the contents of this notebook, execute the following cell and then click the download link below. Note: If you run this notebook on a local Jupyter server, you can expect some of the file path links in the notebook to be broken as they are shaped to our own platform. You can still navigate to the files through the Jupyter file navigator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./Multidimensional+Grids+and+Shared+Memory+for+CUDA+Python+with+Numba.ipynb\n",
      "./add_matrix_stride_solution.py\n",
      "./.ipynb_checkpoints/\n",
      "./.ipynb_checkpoints/Multidimensional+Grids+and+Shared+Memory+for+CUDA+Python+with+Numba-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/matrix_multiply_stride_solution-checkpoint.py\n",
      "./.ipynb_checkpoints/matrix_multiply_solution-checkpoint.py\n",
      "./.ipynb_checkpoints/add_matrix_stride_solution-checkpoint.py\n",
      "./matrix_multiply_solution.py\n",
      "./add_matrix_solution.py\n",
      "./matrix_multiply_stride_solution.py\n",
      "./Multidimensional+Grids+and+Shared+Memory+for+CUDA+Python+with+Numba.py\n",
      "tar: .: file changed as we read it\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf section3.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download the files for this section.](files/section3.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Bank Conflict Free Matrix Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared memory is stored in **banks**. There are 32 banks available for storing shared memory, and memory reads and writes that do not use the same memory bank can perform simultaneously. When parallel threads attempt to access memory in the same bank, we call this a **bank conflict**, which results in the operations being serialized. Even with bank conflicts, shared memory is very fast, but creating memory access patterns that avoid bank conflict are a way to further optimize your applications.\n",
    "\n",
    "We will only give an example here, but for more details, consider the [Efficient Matrix Transpose](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, types\n",
    "import numpy as np\n",
    "\n",
    "TILE_DIM = 32\n",
    "BLOCK_ROWS = 8\n",
    "TILE_DIM_PADDED = TILE_DIM + 1  # Read Mark Harris' blog post to find out why this improves performance!\n",
    "                                # https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose_no_bank_conflict(a_in, a_out):\n",
    "    # THIS CODE ASSUMES IT IS RUNNING WITH A BLOCK DIMENSION OF (TILE_SIZE x TILE_SIZE)\n",
    "    # AND INPUT IS A MULTIPLE OF TILE_SIZE DIMENSIONSx\n",
    "    tile = cuda.shared.array((TILE_DIM, TILE_DIM_PADDED), types.int32)\n",
    "\n",
    "    x = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.y\n",
    "    \n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        tile[cuda.threadIdx.y + j, cuda.threadIdx.x] = a_in[y + j, x] # move tile into shared memory\n",
    "\n",
    "    cuda.syncthreads()  # wait for all threads in the block to finish updating shared memory\n",
    "\n",
    "    # Compute transposed offsets\n",
    "    x = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.y\n",
    "\n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        a_out[y + j, x] = tile[cuda.threadIdx.x, cuda.threadIdx.y + j];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       0        1        2 ...     8189     8190     8191]\n",
      " [    8192     8193     8194 ...    16381    16382    16383]\n",
      " [   16384    16385    16386 ...    24573    24574    24575]\n",
      " ...\n",
      " [67084288 67084289 67084290 ... 67092477 67092478 67092479]\n",
      " [67092480 67092481 67092482 ... 67100669 67100670 67100671]\n",
      " [67100672 67100673 67100674 ... 67108861 67108862 67108863]]\n"
     ]
    }
   ],
   "source": [
    "size = 8192\n",
    "a_in = cuda.to_device(np.arange(size*size, dtype=np.int32).reshape((size, size)))\n",
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "print(a_in.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.95 ms ± 111 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "[[       0     8192    16384 ... 67084288 67092480 67100672]\n",
      " [       1     8193    16385 ... 67084289 67092481 67100673]\n",
      " [       2     8194    16386 ... 67084290 67092482 67100674]\n",
      " ...\n",
      " [    8189    16381    24573 ... 67092477 67100669 67108861]\n",
      " [    8190    16382    24574 ... 67092478 67100670 67108862]\n",
      " [    8191    16383    24575 ... 67092479 67100671 67108863]]\n"
     ]
    }
   ],
   "source": [
    "grid_shape = (int(size/TILE_DIM), int(size/TILE_DIM))\n",
    "\n",
    "%timeit tile_transpose_no_bank_conflict[grid_shape,(TILE_DIM, BLOCK_ROWS)](a_in, a_out); cuda.synchronize()\n",
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-py36",
   "language": "python",
   "name": "pytorch-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
