{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numba import cuda, types"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@cuda.jit\n",
    "def mm_shared(a, b, c):\n",
    "    column, row = cuda.grid(2)\n",
    "    sum = 0\n",
    "\n",
    "    # `a_cache` and `b_cache` are already correctly defined\n",
    "    a_cache = cuda.shared.array(block_size, types.int32)\n",
    "    b_cache = cuda.shared.array(block_size, types.int32)\n",
    "\n",
    "    # TODO: use each thread to populate one element each a_cache and b_cache\n",
    "    \n",
    "    for i in range(a.shape[1]):\n",
    "        # TODO: calculate the `sum` value correctly using values from the cache \n",
    "        sum += a_cache[0][0] * b_cache[0][0]\n",
    "        \n",
    "    c[row][column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Leave the values in this cell alone\n",
    "M = 128\n",
    "N = 32\n",
    "\n",
    "# Input vectors of MxN and NxM dimensions\n",
    "a = np.arange(M*N).reshape(M,N).astype(np.int32)\n",
    "b = np.arange(M*N).reshape(N,M).astype(np.int32)\n",
    "c = np.zeros((M, M)).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "# NxN threads per block, in 2 dimensions\n",
    "block_size = (N,N)\n",
    "# MxM/NxN blocks per grid, in 2 dimensions\n",
    "grid_size = (int(M/N),int(M/N))\n",
    "\n",
    "print(block_size)\n",
    "print(grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def mm(a, b, c):\n",
    "    col, row = cuda.grid(2)\n",
    "    stride_col, stride_row = cuda.gridsize(2)\n",
    "\n",
    "    # `a_cache` and `b_cache` are already correctly defined\n",
    "    #a_cache = cuda.shared.array(block_size, types.int32)\n",
    "    #b_cache = cuda.shared.array(block_size, types.int32)\n",
    "\n",
    "    for data_row in range(row, a.shape[0], stride_row):\n",
    "        for data_col in range(col, b.shape[1], stride_col):\n",
    "            sum = 0\n",
    "            for i in range(a.shape[1]):\n",
    "                sum += a[data_row][i] * b[i][data_col]\n",
    "                \n",
    "            c[data_row][data_col] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def mm_shared(A, B, C):\n",
    "    \"\"\"\n",
    "    使用Shared Memory的矩阵乘法 C = A * B\n",
    "    \"\"\"\n",
    "    # 在Shared Memory中定义向量\n",
    "    # 向量可被整个Block的所有Thread共享\n",
    "    # 必须声明向量大小和数据类型\n",
    "    sA = cuda.shared.array(block_size, types.int32)\n",
    "    sB = cuda.shared.array(block_size, types.int32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    row = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    col = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "    \n",
    "    if row >= C.shape[0] and col >= C.shape[1]:\n",
    "        # 当(x, y)越界时退出\n",
    "        return\n",
    "\n",
    "    tmp = 0\n",
    "    BLOCK_SIZE = block_size[0]\n",
    "    # 以一个 BLOCK_SIZE x BLOCK_SIZE 为单位\n",
    "    for m in range(math.ceil(A.shape[1] / BLOCK_SIZE)):\n",
    "        sA[tx, ty] = A[row, ty + m * BLOCK_SIZE]\n",
    "        sB[tx, ty] = B[tx + m * BLOCK_SIZE, col]\n",
    "        # 线程同步，等待Block中所有Thread预加载结束\n",
    "        # 该函数会等待所有Thread执行完之后才执行下一步\n",
    "        cuda.syncthreads()\n",
    "        # 此时已经将A和B的子矩阵拷贝到了sA和sB\n",
    "\n",
    "        # 计算Shared Memory中的向量点积\n",
    "        # 直接从Shard Memory中读取数据的延迟很低\n",
    "        for n in range(BLOCK_SIZE):\n",
    "            tmp += sA[tx, n] * sB[n, ty]\n",
    "\n",
    "        # 线程同步，等待Block中所有Thread计算结束\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    # 循环后得到每个BLOCK的点积之和\n",
    "    C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 µs ± 436 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# There's no need to update this kernel launch\n",
    "%timeit mm_shared[grid_size, block_size](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the contents in this cell\n",
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 µs ± 455 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mm[grid_size, block_size](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the contents in this cell\n",
    "from numpy import testing\n",
    "solution = a@b\n",
    "output_1 = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output_1, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1333248   1333744   1334240 ...   1395248   1395744   1396240]\n",
      " [  3364864   3366384   3367904 ...   3554864   3556384   3557904]\n",
      " [  5396480   5399024   5401568 ...   5714480   5717024   5719568]\n",
      " ...\n",
      " [255285248 255413744 255542240 ... 271347248 271475744 271604240]\n",
      " [257316864 257446384 257575904 ... 273506864 273636384 273765904]\n",
      " [259348480 259479024 259609568 ... 275666480 275797024 275927568]]\n"
     ]
    }
   ],
   "source": [
    "print(output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37664bittf2condab62182d879ce4f808a6d106065014392"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
